
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Mirror-Aware Neural Humans üèÉüèªü™û</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://danielajisafe.github.io/mirror-aware-neural-humans/img/Title_slide.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1296">
    <meta property="og:image:height" content="840">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://danielajisafe.github.io/mirror-aware-neural-humans/"/>
    <meta property="og:title" content="Mirror-Aware Neural Humans" />
    <meta property="og:description" content="Human motion capture either requires multi-camera systems or is unreliable using single-view input due to depth ambiguities. Meanwhile, mirrors are readily available in urban environments and form an affordable alternative by recording two views with only a single camera. However, the mirror setting poses the additional challenge of handling occlusions of real and mirror image. Going beyond existing mirror approaches for 3D human pose estimation, we utilize mirrors for learning a complete body model, including shape and dense appearance. Our main contributions are extending articulated neural radiance fields to include a notion of a mirror, making it sample-efficient over potential occlusion regions. Together, our contributions realize a consumer-level 3D motion capture system that starts from off-the-shelf 2D poses by automatically calibrating the camera, estimating mirror orientation, and subsequently lifting 2D keypoint detections to 3D skeleton pose that is used to condition the mirror-aware NeRF. We empirically demonstrate the benefit of learning a body model and accounting for occlusion in challenging mirror scenes." />

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Mirror-Aware Neural Humans" />
    <meta name="twitter:description" content="Human motion capture either requires multi-camera systems or is unreliable using single-view input due to depth ambiguities. Meanwhile, mirrors are readily available in urban environments and form an affordable alternative by recording two views with only a single camera. However, the mirror setting poses the additional challenge of handling occlusions of real and mirror image. Going beyond existing mirror approaches for 3D human pose estimation, we utilize mirrors for learning a complete body model, including shape and dense appearance. Our main contributions are extending articulated neural radiance fields to include a notion of a mirror, making it sample-efficient over potential occlusion regions. Together, our contributions realize a consumer-level 3D motion capture system that starts from off-the-shelf 2D poses by automatically calibrating the camera, estimating mirror orientation, and subsequently lifting 2D keypoint detections to 3D skeleton pose that is used to condition the mirror-aware NeRF. We empirically demonstrate the benefit of learning a body model and accounting for occlusion in challenging mirror scenes." />
    <meta name="twitter:image" content="https://danielajisafe.github.io/mirror-aware-neural-humans/img/Title_slide.png" />


<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üí´</text></svg>">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                </b> <b>Mirror-Aware Neural Humans üèÉüèªü™û</b> </br> 
                <!-- <b>Mip-NeRF 360</b>: Unbounded <br> Mirror-Aware Neural Humans </br>  -->
                <small>
								3DV 2024
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://danielajisafe.github.io/">
                          Daniel Ajisafe
                        </a>
                        </br>The University of British Columbia
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/james-tang-279332196/?originalSubdomain=ca">
                            James Tang
                        </a>
                        </br>The University of British Columbia
                    </li>
                    <li>
                        <a href="https://lemonatsu.github.io/">
                            Shih-Yang Su
                        </a>
                        </br>The University of British Columbia
                    </li><br>
                    <li>
                        <a href="https://bastianwandt.de/">
                            Bastian Wandt
                        </a>
                        </br>Link√∂ping University
                    </li>
                    <li>
                        <a href="https://www.cs.ubc.ca/~rhodin/web/">
                            Helge Rhodin
                        </a>
                        </br>The University of British Columbia
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <!-- <iframe src="docs/Main_paper.pdf" style="width:1000px; height:800px;" frameborder="0" allowfullscreen></iframe> -->
                            <!-- <embed src="docs/Main_paper.pdf" type="application/pdf"> -->
                            <a href="https://arxiv.org/abs/2309.04750">
                            <image src="img/mirror_paper_frontpage.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <!-- <embed src="docs/Supp.pdf" type="application/pdf"> -->
                            <a href="docs/Supp.pdf">
                            <image src="img/supp_front_page.png" height="60px">
                                <h4><strong>Supplementary</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://youtu.be/qEVYFTiK0MA">
                            <!-- <a href="https://youtu.be/i1eaI0f72EI"> -->
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <!-- <li>
                            <a href="http://storage.googleapis.com/gresearch/refraw360/360_v2.zip" target="popup" onclick="window.open('http://storage.googleapis.com/gresearch/refraw360/360_v2.zip','popup','width=600,height=400')">
                            <image src="img/database_icon.png" height="60px">
                                <h4><strong>Dataset</strong></h4>
                            </a>
                        </li> -->
                        <li>
                            <a href="">
                            <image src="img/github.png" height="60px">
                                <h4><strong>(Code) coming soon</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>



        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/gardenvase_720.mp4" type="video/mp4" />
                </video>
						</div>
            <div class="col-md-8 col-md-offset-2">
							<p class="text-center">
							Teaser-Figure
							</p>
						</div>
        </div> -->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Human motion capture either requires multi-camera systems or is unreliable using single-view input due to depth ambiguities. Meanwhile, mirrors are readily available in urban environments and form an affordable alternative by recording two views with only a single camera. However, the mirror setting poses the additional challenge of handling occlusions of real and mirror image. Going beyond existing mirror approaches for 3D human pose estimation, we utilize mirrors for learning a complete body model, including shape and dense appearance. Our main contributions are extending articulated neural radiance fields to include a notion of a mirror, making it sample-efficient over potential occlusion regions. Together, our contributions realize a consumer-level 3D motion capture system that starts from off-the-shelf 2D poses by automatically calibrating the camera, estimating mirror orientation, and subsequently lifting 2D keypoint detections to 3D skeleton pose that is used to condition the mirror-aware NeRF. We empirically demonstrate the benefit of learning a body model and accounting for occlusion in challenging mirror scenes. 
                </p>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://www.youtube.com/embed/qEVYFTiK0MA?si=HG_NSOaravZdgBEM" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                        <!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/i1eaI0f72EI?si=aOpYUOWFBhKMqw9M" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe> -->
                        <!-- <iframe src="https://www.youtube.com/embed/i1eaI0f72EI?si=aOpYUOWFBhKMqw9M" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe> -->
                    </div>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Overview
                </h3>
                <image src="img/overview_figure.png" style="max-width: 100%;"></image>
                <!-- <p class="text-justify">
                    xxxx
                </p> -->
            </div>
        </div>

        <!-- Right-click and use "inspect" to debug code underlying webpages intuitively   -->
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea style="align-content:left" id="bibtex" class="form-control" readonly>

                @article{ajisafe2023mirror,
                    title={Mirror-Aware Neural Humans},
                    author={Ajisafe, Daniel and Tang, James and Su, Shih-Yang and Wandt, Bastian and Rhodin, Helge},
                    journal={arXiv preprint arXiv:2309.04750},
                    year={2023}
                    }
                    </textarea>
                </div>
                <h3>
                    MirrorHuman-eval dataset
                </h3>
                    <div class="form-group col-md-10 col-md-offset-1">
                        <textarea style="align-content:left" id="bibtex1" class="form-control" readonly>

                    @inproceedings{fang2021mirrored,
                        title={Reconstructing 3D Human Pose by Watching Humans in the Mirror},
                        author={Fang, Qi and Shuai, Qing and Dong, Junting and Bao, Hujun and Zhou, Xiaowei},
                        booktitle={CVPR},
                        year={2021}
                    }
                        </textarea>
                    </div>

            </div>
        </div>

        
    
        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    MirrorHuman-eval dataset
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
        
                @inproceedings{fang2021mirrored,
                    title={Reconstructing 3D Human Pose by Watching Humans in the Mirror},
                    author={Fang, Qi and Shuai, Qing and Dong, Junting and Bao, Hujun and Zhou, Xiaowei},
                    booktitle={CVPR},
                    year={2021}
                    }
                    </textarea>
                </div>
            </div>
        </div> -->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                We are very grateful for the helpful comments from <a href="https://csprofkgd.github.io/">Kosta Derpanis</a> on an earlier draft of the paper, as well as insightful feedback from current and past members of the <a href="https://www.cs.ubc.ca/~rhodin/web/#Team">Visual AI Lab</a> team.  
                We extend gratitude to <a href="https://yu-frank.github.io/">Frank Yu</a> for graciously assisting with data pre-processing for our baseline evaluation.
                We also say thank you to the <a href="https://ccdb.alliancecan.ca/">Advanced Research Computing</a> at the University of British Columbia and <a href="https://docs.alliancecan.ca/wiki/Getting_started">Compute Canada</a> for providing computational resources.
                Awesome thanks to generous performers (<a href="https://www.youtube.com/@Charissahoo">Charissa Hoo</a>, <a href="">U Limn</a>, <a href="">Eclipse</a>, <a href="">Onken</a> and <a href="https://www.youtube.com/@Bobylien">Bobylien</a>) who gave consent to use their videos for our research. 
                Music credits to <a href="https://pixabay.com/users/soulprodmusic-30064790/?utm_source=link-attribution&utm_medium=referral&utm_campaign=music&utm_content=124265">Oleg Fedak</a> via <a href="https://pixabay.com/">Pixabay</a>.
                    <br/> <br/>

                    <!-- <ul>
                        <li> <a href="https://www.youtube.com/@Charissahoo">Charissa Hoo</a> </li>
                        <li> <a href="">U Limn</a> </li>
                        <li> <a href="">Eclipse</a> </li>
                        <li> <a href="">Onken</a> </li>
                        <li> <a href="https://www.youtube.com/@Bobylien">Bobylien</a> </li>  
                      </ul>  -->
                    
                The website template was borrowed from <a href="http://mgharbi.com/">Micha√´l Gharbi</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
