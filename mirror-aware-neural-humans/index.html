
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>mip-NeRF 360</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://jonbarron.info/mipnerf360/img/gardenvase.jpg">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1296">
    <meta property="og:image:height" content="840">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://jonbarron.info/mipnerf360/"/>
    <meta property="og:title" content="Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields" />
    <meta property="og:description" content="Though neural radiance fields (NeRF) have demonstrated impressive view synthesis results on objects and small bounded regions of space, they struggle on 'unbounded' scenes, where the camera may point in any direction and content may exist at any distance. In this setting, existing NeRF-like models often produce blurry or low-resolution renderings (due to the unbalanced detail and scale of nearby and distant objects), are slow to train, and may exhibit artifacts due to the inherent ambiguity of the task of reconstructing a large scene from a small set of images. We present an extension of mip-NeRF (a NeRF variant that addresses sampling and aliasing) that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes. Our model, which we dub 'mip-NeRF 360' as we target scenes in which the camera rotates 360 degrees around a point, reduces mean-squared error by 54% compared to mip-NeRF, and is able to produce realistic synthesized views and detailed depth maps for highly intricate, unbounded real-world scenes." />

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields" />
    <meta name="twitter:description" content="Though neural radiance fields (NeRF) have demonstrated impressive view synthesis results on objects and small bounded regions of space, they struggle on 'unbounded' scenes, where the camera may point in any direction and content may exist at any distance. In this setting, existing NeRF-like models often produce blurry or low-resolution renderings (due to the unbalanced detail and scale of nearby and distant objects), are slow to train, and may exhibit artifacts due to the inherent ambiguity of the task of reconstructing a large scene from a small set of images. We present an extension of mip-NeRF (a NeRF variant that addresses sampling and aliasing) that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes. Our model, which we dub 'mip-NeRF 360' as we target scenes in which the camera rotates 360 degrees around a point, reduces mean-squared error by 54% compared to mip-NeRF, and is able to produce realistic synthesized views and detailed depth maps for highly intricate, unbounded real-world scenes." />
    <meta name="twitter:image" content="https://jonbarron.info/mipnerf360/img/gardenvase.jpg" />


<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ’«</text></svg>">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                </b> <b>Mirror-Aware Neural Humans</b> </br> 
                <!-- <b>Mip-NeRF 360</b>: Unbounded <br> Mirror-Aware Neural Humans </br>  -->
                <!-- <small>
								CVPR 2022 (Oral Presentation)
                </small> -->
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://danielajisafe.github.io/">
                          Daniel Ajisafe
                        </a>
                        </br>The University of British Columbia
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/james-tang-279332196/?originalSubdomain=ca">
                            James Tang
                        </a>
                        </br>The University of British Columbia
                    </li>
                    <li>
                        <a href="https://lemonatsu.github.io/">
                            Shih-Yang Su
                        </a>
                        </br>The University of British Columbia
                    </li><br>
                    <li>
                        <a href="https://bastianwandt.de/">
                            Bastian Wandt
                        </a>
                        </br>LinkÃ¶ping University
                    </li>
                    <li>
                        <a href="https://www.cs.ubc.ca/~rhodin/web/">
                            Helge Rhodin
                        </a>
                        </br>The University of British Columbia
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="">
                            <image src="img/mirror_paper_frontpage.png" height="60px">
                                <h4><strong>(Paper) coming soon</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://youtu.be/i1eaI0f72EI">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <!-- <li>
                            <a href="http://storage.googleapis.com/gresearch/refraw360/360_v2.zip" target="popup" onclick="window.open('http://storage.googleapis.com/gresearch/refraw360/360_v2.zip','popup','width=600,height=400')">
                            <image src="img/database_icon.png" height="60px">
                                <h4><strong>Dataset</strong></h4>
                            </a>
                        </li> -->
                        <li>
                            <a href="">
                            <image src="img/github.png" height="60px">
                                <h4><strong>(Code) coming soon</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>



        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/gardenvase_720.mp4" type="video/mp4" />
                </video>
						</div>
            <div class="col-md-8 col-md-offset-2">
							<p class="text-center">
							Teaser-Figure
							</p>
						</div>
        </div> -->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Human motion capture either requires multi-camera systems or is unreliable using single-view input due to depth ambiguities. Meanwhile, mirrors are readily available in urban environments and form an affordable alternative by recording two views with only a single camera. However, the mirror setting poses the additional challenge of handling occlusions of real and mirror image. Going beyond existing mirror approaches for 3D human pose estimation, we utilize mirrors for learning a complete body model, including shape and dense appearance. Our main contributions are extending articulated neural radiance fields to include a notion of a mirror, making it sample-efficient over potential occlusion regions. Together, our contributions realize a consumer-level 3D motion capture system that starts from off-the-shelf 2D poses by automatically calibrating the camera, estimating mirror orientation, and subsequently lifting 2D keypoint detections to 3D skeleton pose that is used to condition the mirror-aware NeRF. We empirically demonstrate the benefit of learning a body model and accounting for occlusion in challenging mirror scenes. 
                </p>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://youtu.be/i1eaI0f72EI" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Overview
                </h3>
                <image src="img/overview_figure.png" style="max-width: 100%;"></image>
                <!-- <p class="text-justify">
                    xxxx
                </p> -->
            </div>
        </div>

            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@article{xxxx,
    title={Mirror-Aware Neural Humans},
    author={xxx},
    journal={Arxiv},
    year={2023}
}
</textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                We are very grateful for the helpful comments from <a href="https://csprofkgd.github.io/">Kosta Derpanis</a> on an earlier draft of the paper, as well as insightful feedback from current and past members of the <a href="https://www.cs.ubc.ca/~rhodin/web/#Team">Visual AI Lab</a> team.  
                We extend gratitude to <a href="https://yu-frank.github.io/">Frank Yu</a> for graciously assisting with data pre-processing for our baseline evaluation.
                We also say thank you to the <a href="https://ccdb.alliancecan.ca/">Advanced Research Computing</a> at the University of British Columbia and <a href="https://docs.alliancecan.ca/wiki/Getting_started">Compute Canada</a> for providing computational resources.
                Awesome thanks to generous performers (<a href="https://www.youtube.com/@Charissahoo">Charissa Hoo</a>, <a href="">U Limn</a>, <a href="">Eclipse</a>, <a href="">Onken</a> and <a href="https://www.youtube.com/@Bobylien">Bobylien</a>) who gave consent to use their videos for our research. 
                Music credits to <a href="https://pixabay.com/users/soulprodmusic-30064790/?utm_source=link-attribution&utm_medium=referral&utm_campaign=music&utm_content=124265">Oleg Fedak</a> via <a href="https://pixabay.com/">Pixabay</a>.
                    <br/> <br/>

                    <!-- <ul>
                        <li> <a href="https://www.youtube.com/@Charissahoo">Charissa Hoo</a> </li>
                        <li> <a href="">U Limn</a> </li>
                        <li> <a href="">Eclipse</a> </li>
                        <li> <a href="">Onken</a> </li>
                        <li> <a href="https://www.youtube.com/@Bobylien">Bobylien</a> </li>  
                      </ul>  -->
                    
                The website template was borrowed from <a href="http://mgharbi.com/">MichaÃ«l Gharbi</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
